# -*- coding: utf-8 -*-
"""Copy of CSE432_Inclass_exe_Ch4.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dX5blw039IHpB6qq4_TaRfGpTacL6sCr

# Step 1: Generate the Dataset

In this part we want to generate the data point for training.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline
plt.rcParams['figure.figsize']=(10.0,8.0)
plt.rcParams['image.interpolation']='nearest'
plt.rcParams['image.cmap']='gray'


# Softmax(Relu(x*W+b)*W2+b2)

np.random.seed(0)
N=100 # number of data point for each class
D=2  # number of demension
K=3  # number of class
X=np.zeros((N*K,D))#2-D matrix 300 by 2
Y=np.zeros(N*K,dtype='uint8')#1-D vector, Label
for j in range(K):
  ix=range(N*j,N*(j+1))
  r=np.linspace(0.0,1,N)
  t=np.linspace(j*4,(j+1)*4,N)+np.random.randn(N)*0.2
  X[ix]=np.c_[r*np.sin(t),r*np.cos(t)]
  Y[ix]=j
fig=plt.figure()
plt.scatter(X[:,0],X[:,1],c=Y,s=40,cmap=plt.cm.Spectral)
plt.ylim([-1,1])
plt.xlim([-1,1])

print(X.shape)
print(Y.shape)
W=0.01*np.random.randn(D,K)
print(W.shape)

b=np.zeros((1,K))
scores=np.dot(X,W)
print(scores.shape)
print(b)
print(b.shape)

"""# Step 2: Linear classifier (one layer NN)

In this part, we want to implement a linear classifier.

## The first step is to initialize the parameter W & b

"""

Hi = 100
W=0.01*np.random.randn(D,Hi)
b=np.zeros((1,Hi))
#print(W)
W2=0.01*np.random.randn(Hi,K)
b2=np.zeros((1,K))

"""# Please read the "Chapter 4.4 Neural Network Demos V2" (Page 37) to understand the following code."""

print(W.shape)
print(X.shape)

step_size=1e-0 # learning rate
reg=1e-3 #lambda

num_examples=X.shape[0]
for i in range(200):
  #forward propagation
  #y=(x*w+b)
  h1=np.dot(X,W)
  h2=b+h1
  h3 = np.maximum(0, h2)
  h4 = np.dot(h3, W2)
  h5 = h4 + b2
  #softmax()
  exp_scores=np.exp(h2)
  s=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)

  #loss
  l1_all=-np.log(s[range(num_examples),Y])
  l1=np.sum(l1_all)/num_examples
  l3=np.sum(W*W)
  l4 = reg*l3
  l5 = np.sum(W2*W2)
  l6 = reg*l5
  l2 = l1+l4
  l2=l2+l6
  if i%10==0:
    print("iteration %d: loss %f" % (i,loss))

  #back propagation
  s[range(num_examples),Y]-=1 #we can not use dh2=s[range(num_examples),Y]-1
  dh5=s
  dh5/=num_examples
  dh4=dh5
  db2=np.sum(dh5, axis=0, keepdims=True)
  dh3 = np.dot(dh4, W2.T)
  dW2 = np.dot(h3.T, dh4)
  dh2 = dh3
  dh2[h2<=0] = 0
  db = dh2
  # ///
  dh2/=num_examples
  dh1=dh2
  dw=np.dot(X.T,dh1)
  db=np.sum(dh2, axis=0,keepdims=True)
  dl2=reg
  dw+=2*dl2*W

  W+=-step_size*dw
  b+=-step_size*db
  if i == 190:
    print("Shape")
    print(dh1.shape)
    print(dh2.shape)
    print(dw.shape)
    print(db.shape)

scores=np.maximum(X.dot(W)+b, 0)
scores=scores*W2+b2
predicted_class=np.argmax(scores, axis=1)
print("training accuracy: %.2f" % (np.mean(predicted_class==Y)))

h=0.02
x_min, x_max=X[:,0].min()-1,X[:,0].max()+1
y_min, y_max=X[:,1].min() -1, X[:,1].max()+1

xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))
Z=np.dot(np.c_[xx.ravel(),yy.ravel()],W)+b
Z=np.argmax(Z,axis=1)
Z=Z.reshape(xx.shape)
fig=plt.figure()

plt.contourf(xx,yy,Z, cmap=plt.cm.Spectral,alpha=0.8)
plt.scatter(X[:,0],X[:,1],c=Y,s=40,cmap=plt.cm.Spectral)
plt.ylim(xx.min(),xx.max())
plt.xlim(yy.min(),yy.max())

"""# MNIST DATASET

https://colab.research.google.com/drive/1smEmXB196RIx5Q5c-yaR2MNLDRHO9PTM?usp=sharing#scrollTo=x1JRTuxiwqVa
"""